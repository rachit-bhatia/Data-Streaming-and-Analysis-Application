{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1.3: Producer 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing records..\n",
      "Success publishing message. Data: {'latitude': -35.961, 'longitude': 141.089, 'confidence': 83.0, 'surface_temperature_celcius': 56.0, 'created_time': '08:37:30', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -38.1544, 'longitude': 143.7961, 'confidence': 81.0, 'surface_temperature_celcius': 54.0, 'created_time': '13:51:47', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -36.4315, 'longitude': 141.1987, 'confidence': 76.0, 'surface_temperature_celcius': 49.0, 'created_time': '13:05:14', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -37.9318, 'longitude': 143.2083, 'confidence': 69.0, 'surface_temperature_celcius': 44.0, 'created_time': '08:47:38', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -37.6865, 'longitude': 148.5154, 'confidence': 66.0, 'surface_temperature_celcius': 50.0, 'created_time': '23:31:06', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -37.9284, 'longitude': 142.5465, 'confidence': 78.0, 'surface_temperature_celcius': 51.0, 'created_time': '06:52:14', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -37.8475, 'longitude': 147.2512, 'confidence': 76.0, 'surface_temperature_celcius': 50.0, 'created_time': '13:59:33', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -36.3883, 'longitude': 143.7526, 'confidence': 56.0, 'surface_temperature_celcius': 39.0, 'created_time': '18:53:28', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -36.4622, 'longitude': 142.4773, 'confidence': 83.0, 'surface_temperature_celcius': 57.0, 'created_time': '16:01:13', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -36.0765, 'longitude': 146.4896, 'confidence': 69.0, 'surface_temperature_celcius': 46.0, 'created_time': '00:41:02', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -36.7536, 'longitude': 141.6092, 'confidence': 70.0, 'surface_temperature_celcius': 45.0, 'created_time': '02:21:10', 'producer_identifier': 'producer_3'}\n",
      "Success publishing message. Data: {'latitude': -36.4617, 'longitude': 144.6381, 'confidence': 70.0, 'surface_temperature_celcius': 48.0, 'created_time': '13:44:02', 'producer_identifier': 'producer_3'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m data_doc \u001b[39m=\u001b[39m add_additional_data(random_data_doc)\n\u001b[1;32m     86\u001b[0m publish_message(producer, topic_name, data_doc)  \u001b[39m#publish chosen data document\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m sleep(publish_interval)  \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from kafka3 import KafkaProducer\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import time\n",
    "import json\n",
    "\n",
    "\n",
    "ip_address = \"10.192.68.151\" \n",
    "\n",
    "topic_name = 'climate_hotspot'  #use the same topic for all producers\n",
    "producer_identifier = \"producer_3\"\n",
    "\n",
    "\n",
    "def prepare_streaming_data():\n",
    "        \n",
    "    #read csv file and extract rows\n",
    "    hotspot_aqua_csv = pd.read_csv(\"hotspot_TERRA_streaming.csv\").iterrows()\n",
    "\n",
    "    hotspot_aqua_data = []\n",
    "\n",
    "    #get all streaming data rows as dictionaries (json) and store them\n",
    "    for id, data_row in hotspot_aqua_csv:\n",
    "        row_dict = data_row.to_dict()\n",
    "        hotspot_aqua_data.append(row_dict)\n",
    "\n",
    "    return hotspot_aqua_data\n",
    "\n",
    "#add additional attributes to each data document\n",
    "def add_additional_data(data_document):\n",
    "\n",
    "    #generate random time\n",
    "    hour = random.randint(0, 23)\n",
    "    minutes = random.randint(0, 59)\n",
    "    seconds = random.randint(0, 59)\n",
    "    random_time = time(hour, minutes, seconds)\n",
    "\n",
    "    #add random time and producer identifier label\n",
    "    data_document[\"created_time\"] = random_time.strftime(\"%H:%M:%S\")    #convert to string to make it json serialisable\n",
    "    data_document[\"producer_identifier\"] = producer_identifier\n",
    "\n",
    "    return data_document\n",
    "\n",
    "\n",
    "def publish_message(producer_instance, kafka_topic, data):\n",
    "    try:\n",
    "        producer_instance.send(kafka_topic, value=data)  #send the message to the specified topic\n",
    "        producer_instance.flush()  #make sure that the message is actually sent to Kafka before moving on\n",
    "\n",
    "        print('Success publishing message. Data: ' + str(data))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Error! Message could not be published')\n",
    "        print(str(e))\n",
    "        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        #create producer at given address port\n",
    "        _producer = KafkaProducer(bootstrap_servers=[f'{ip_address}:9092'],\n",
    "                                  value_serializer=lambda x: json.dumps(x).encode('ascii'),  #encode as bytes because Kafka messages are sent as byte arrays\n",
    "                                  api_version=(0, 10))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Could not create producer. Error connecting to Kafka.')\n",
    "        print(str(e))\n",
    "\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    streaming_climate_data = prepare_streaming_data()  #get all streaming data as dictionary objects (like json)\n",
    "    print('Publishing records..')\n",
    "    producer = connect_kafka_producer()\n",
    "    \n",
    "    while True:\n",
    "        #publish data in random intervals (1 to 20 seconds)\n",
    "        publish_interval = random.randint(1, 20)  \n",
    "\n",
    "        #choose document randomly\n",
    "        random_doc_id = random.randint(0, len(streaming_climate_data)-1) \n",
    "        random_data_doc = streaming_climate_data[random_doc_id]\n",
    "\n",
    "        data_doc = add_additional_data(random_data_doc)\n",
    "        publish_message(producer, topic_name, data_doc)  #publish chosen data document\n",
    "\n",
    "        sleep(publish_interval)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
